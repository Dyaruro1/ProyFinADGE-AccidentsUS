{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 12020309,
          "sourceType": "datasetVersion",
          "datasetId": 7562543
        }
      ],
      "dockerImageVersionId": 31040,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "3f619533",
      "cell_type": "markdown",
      "source": [
        "# Exploratory Data Analysis (EDA) Distribuido para ML en Spark/Cluster\n",
        "Este notebook realiza un EDA eficiente y escalable usando PySpark y librerías distribuidas como Modin, ideales para grandes volúmenes de datos en clústeres Spark/Hadoop."
      ],
      "metadata": {
        "id": "3f619533"
      }
    },
    {
      "id": "9889d2ca",
      "cell_type": "code",
      "source": [
        "# Inicializar SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import modin.pandas as pd\n",
        "import seaborn as sns\n",
        "spark = SparkSession.builder.appName(\"US_Accidents_EDA\").getOrCreate()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-06-01T17:01:33.460509Z",
          "iopub.execute_input": "2025-06-01T17:01:33.460842Z",
          "iopub.status.idle": "2025-06-01T17:01:46.224363Z",
          "shell.execute_reply.started": "2025-06-01T17:01:33.460787Z",
          "shell.execute_reply": "2025-06-01T17:01:46.223145Z"
        },
        "id": "9889d2ca",
        "outputId": "6e0b897d-9621-4f3d-9cab-b84217123112"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/06/01 17:01:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "id": "7d869797",
      "cell_type": "code",
      "source": [
        "# Cargar el dataset en Spark DataFrame\n",
        "df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/data/Accidents_USA_March23.csv\")\n",
        "df.cache()\n",
        "df.show(5)"
      ],
      "metadata": {
        "trusted": true,
        "id": "7d869797"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7aef8779",
      "cell_type": "code",
      "source": [
        "# Dimensiones y tipos de datos\n",
        "df.printSchema()\n",
        "print(f\"Filas: {df.count()}, Columnas: {len(df.columns)}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "7aef8779"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "fb6caf0f",
      "cell_type": "code",
      "source": [
        "# Estadísticas descriptivas\n",
        "df.describe().show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "fb6caf0f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f804caa9",
      "cell_type": "markdown",
      "source": [
        "## Estadísticas descriptivas y distribución de la variable objetivo"
      ],
      "metadata": {
        "id": "f804caa9"
      }
    },
    {
      "id": "4a505029",
      "cell_type": "code",
      "source": [
        "# Conteo de severidad de accidentes (distribución de la variable objetivo)\n",
        "severity_pd = df.groupBy('Severity').count().orderBy('Severity').toPandas()\n",
        "sns.barplot(x='Severity', y='count', data=severity_pd)\n",
        "plt.title('Distribución de Severidad de Accidentes')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "4a505029"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7471fe91",
      "cell_type": "code",
      "source": [
        "# Estadísticas descriptivas de variables numéricas\n",
        "num_features = [field.name for field in df.schema.fields if field.dataType.typeName() in ['integer', 'double']]\n",
        "df.select(num_features).describe().show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "7471fe91"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "d1c5f113",
      "cell_type": "markdown",
      "source": [
        "# Limpieza de valores nulos y preparacion de datos\n"
      ],
      "metadata": {
        "id": "d1c5f113"
      }
    },
    {
      "id": "2f22ca30",
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, count, when, lit, to_timestamp\n",
        "\n",
        "# Eliminar columnas con demasiados nulos\n",
        "cols_to_drop = [\n",
        "    'End_Lat', 'End_Lng', 'Precipitation(in)', 'Wind_Chill(F)', 'Airport_Code'\n",
        "]\n",
        "df = df.drop(*cols_to_drop)\n",
        "\n",
        "# Imputar columnas numéricas con la mediana\n",
        "def impute_median(df, cols):\n",
        "    for col_name in cols:\n",
        "        median_val = df.approxQuantile(col_name, [0.5], 0.01)[0] if col_name in df.columns else None\n",
        "        if median_val is not None:\n",
        "            df = df.withColumn(col_name, when(col(col_name).isNull(), lit(median_val)).otherwise(col(col_name)))\n",
        "    return df\n",
        "\n",
        "num_cols = [\n",
        "    'Wind_Speed(mph)', 'Visibility(mi)', 'Humidity(%)', 'Temperature(F)', 'Pressure(in)'\n",
        "]\n",
        "df = impute_median(df, num_cols)\n",
        "\n",
        "# Imputar columnas categóricas con la moda\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "def impute_mode(df, cols):\n",
        "    for col_name in cols:\n",
        "        if col_name in df.columns:\n",
        "            mode_row = df.groupBy(col_name).count().orderBy(F.desc('count')).first()\n",
        "            mode_val = mode_row[0] if mode_row else 'Unknown'\n",
        "            df = df.withColumn(col_name, when(col(col_name).isNull(), lit(mode_val)).otherwise(col(col_name)))\n",
        "    return df\n",
        "\n",
        "cat_cols = [\n",
        "    'Weather_Condition', 'Wind_Direction', 'Weather_Timestamp',\n",
        "    'Nautical_Twilight', 'Civil_Twilight', 'Sunrise_Sunset', 'Astronomical_Twilight',\n",
        "    'Street', 'Timezone', 'Zipcode', 'City'\n",
        "]\n",
        "df = impute_mode(df, cat_cols)\n",
        "\n",
        "# Verificar que no queden nulos\n",
        "df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "2f22ca30"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "02e87aac",
      "cell_type": "code",
      "source": [
        "# Imputación de nulos en columnas de fecha con la mediana o valor fijo si la columna es nula\n",
        "\n",
        "from pyspark.sql.functions import col, when, lit, to_timestamp, unix_timestamp\n",
        "\n",
        "def impute_datetime_median_spark(df, cols):\n",
        "    for col_name in cols:\n",
        "        if col_name in df.columns:\n",
        "            # Convertir a timestamp si es necesario\n",
        "            df = df.withColumn(col_name, to_timestamp(col(col_name)))\n",
        "            # Convertir a long\n",
        "            df = df.withColumn(col_name + '_long', unix_timestamp(col(col_name)))\n",
        "            notnull_count = df.filter(col(col_name + '_long').isNotNull()).count()\n",
        "            if notnull_count > 0:\n",
        "                median_val = df.approxQuantile(col_name + '_long', [0.5], 0.01)[0]\n",
        "                df = df.withColumn(\n",
        "                    col_name + '_long',\n",
        "                    when(col(col_name + '_long').isNull(), lit(median_val)).otherwise(col(col_name + '_long'))\n",
        "                )\n",
        "                # Volver a timestamp\n",
        "                df = df.withColumn(col_name, (col(col_name + '_long')).cast('timestamp'))\n",
        "            else:\n",
        "                df = df.withColumn(\n",
        "                    col_name,\n",
        "                    when(col(col_name).isNull(), lit('1970-01-01 00:00:00')).otherwise(col(col_name))\n",
        "                )\n",
        "            # Eliminar columna auxiliar\n",
        "            df = df.drop(col_name + '_long')\n",
        "    return df\n",
        "\n",
        "fecha_cols = ['Start_Time', 'End_Time', 'Weather_Timestamp']\n",
        "df = impute_datetime_median_spark(df, fecha_cols)\n",
        "\n",
        "# Verificar que no queden nulos en fechas\n",
        "df.select([F.count(F.when(col(c).isNull(), c)).alias(c) for c in fecha_cols]).show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "02e87aac"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5600a5e5",
      "cell_type": "markdown",
      "source": [
        "# Visualización de EDA distribuido: Graficar resultados agregados de Spark\n",
        "Para visualizar los resultados de Spark, exportamos muestras o agregados a pandas y graficamos con matplotlib/seaborn. Esto permite análisis visual incluso en entornos distribuidos."
      ],
      "metadata": {
        "id": "5600a5e5"
      }
    },
    {
      "id": "71a62759",
      "cell_type": "code",
      "source": [
        "# Top 10 ciudades con más accidentes\n",
        "city_pd = df.groupBy('City').count().orderBy('count', ascending=False).limit(10).toPandas()\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x='City', y='count', data=city_pd)\n",
        "plt.title('Top 10 ciudades con más accidentes')\n",
        "plt.ylabel('Número de accidentes')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "71a62759"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3408c550",
      "cell_type": "code",
      "source": [
        "# Top 15 estados con más accidentes\n",
        "state_pd = df.groupBy('State').count().orderBy('count', ascending=False).limit(15).toPandas()\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x='State', y='count', data=state_pd)\n",
        "plt.title('Accidentes por estado (Top 15)')\n",
        "plt.ylabel('Número de accidentes')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "3408c550"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "14df86d9",
      "cell_type": "code",
      "source": [
        "# Distribución de accidentes por franja horaria (requiere conversión de fecha)\n",
        "from pyspark.sql.functions import hour, col\n",
        "if 'Start_Time' in df.columns:\n",
        "    df = df.withColumn('hour', hour(col('Start_Time')))\n",
        "    hour_pd = df.groupBy('hour', 'Severity').count().orderBy('hour', 'Severity').toPandas()\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.barplot(x='hour', y='count', hue='Severity', data=hour_pd)\n",
        "    plt.title('Accidentes por hora y severidad')\n",
        "    plt.xlabel('Hora')\n",
        "    plt.ylabel('Número de accidentes')\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "14df86d9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "655b3b4c",
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Ejemplo: Gráfico de barras de la distribución de severidad\n",
        "severity_pd = df.groupBy('Severity').count().orderBy('Severity').toPandas()\n",
        "sns.barplot(x='Severity', y='count', data=severity_pd)\n",
        "plt.title('Distribución de Severidad de Accidentes')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "655b3b4c"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3d363d80",
      "cell_type": "code",
      "source": [
        "# barplot de Distance(mi) vs Severity\n",
        "boxplot_pd = df.groupBy('Severity').agg(F.expr('percentile_approx(`Distance(mi)`, 0.5)').alias('median'),\n",
        "                                        F.avg('Distance(mi)').alias('mean')).orderBy('Severity').toPandas()\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.barplot(x='Severity', y='median', data=boxplot_pd)\n",
        "plt.title('Mediana de Distance(mi) por Severity')\n",
        "plt.ylabel('Distance(mi) (mediana)')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "3d363d80"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b742acca",
      "cell_type": "code",
      "source": [
        "# Heatmap de Weather_Condition vs Severity (Top 10)\n",
        "weather_pd = df.groupBy('Weather_Condition', 'Severity').count().toPandas()\n",
        "top_weather = weather_pd.groupby('Weather_Condition')['count'].sum().nlargest(10).index\n",
        "weather_pd = weather_pd[weather_pd['Weather_Condition'].isin(top_weather)]\n",
        "weather_pivot = weather_pd.pivot(index='Weather_Condition', columns='Severity', values='count').fillna(0)\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(weather_pivot, annot=True, fmt='.0f', cmap='Blues')\n",
        "plt.title('Weather Condition vs Severity (Top 10)')\n",
        "plt.ylabel('Weather_Condition')\n",
        "plt.xlabel('Severity')\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "b742acca"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ca347b65",
      "cell_type": "code",
      "source": [
        "# Gráfico de barras de accidentes por hora y severidad\n",
        "if 'hour' in df.columns:\n",
        "    hour_pd = df.groupBy('hour', 'Severity').count().orderBy('hour', 'Severity').toPandas()\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.barplot(x='hour', y='count', hue='Severity', data=hour_pd)\n",
        "    plt.title('Accidentes por hora y severidad')\n",
        "    plt.xlabel('Hora')\n",
        "    plt.ylabel('Número de accidentes')\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "ca347b65"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "14c8cb27",
      "cell_type": "code",
      "source": [
        "# barplots de variables numéricas vs Severity (además de Distance(mi))\n",
        "for col in ['Temperature(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)']:\n",
        "    boxplot_pd = df.groupBy('Severity').agg(F.expr(f'percentile_approx(`{col}`, 0.5)').alias('median')).orderBy('Severity').toPandas()\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    sns.barplot(x='Severity', y='median', data=boxplot_pd)\n",
        "    plt.title(f'Mediana de {col} por Severity')\n",
        "    plt.ylabel(f'{col} (mediana)')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "14c8cb27"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e779da2f",
      "cell_type": "markdown",
      "source": [
        "# Eliminación de filas con outliers extremos en columnas numéricas\n",
        "Filtramos las filas que tienen valores mayores al percentil 99.5 en 'Distance(mi)', 'Wind_Speed(mph)' y 'Visibility(mi)' para mejorar la calidad del análisis y los modelos de ML."
      ],
      "metadata": {
        "id": "e779da2f"
      }
    },
    {
      "id": "0142922e",
      "cell_type": "code",
      "source": [
        "cols_outliers = ['Distance(mi)', 'Wind_Speed(mph)', 'Visibility(mi)']\n",
        "percentile = 0.995  # Puedes ajustar este valor\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "for col in cols_outliers:\n",
        "    if col in df.columns:\n",
        "        upper = df.approxQuantile(col, [percentile], 0.01)[0]\n",
        "        df = df.filter(F.col(col) <= upper)\n",
        "\n",
        "print(f\"Filas restantes tras eliminar outliers en {cols_outliers}: {df.count()}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "0142922e"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "a038b745",
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering sobre la columna 'Description'\n",
        "La columna 'Description' contiene información textual sobre el accidente. Vamos a explorarla y extraer features útiles para modelos de ML."
      ],
      "metadata": {
        "id": "a038b745"
      }
    },
    {
      "id": "00139c3d",
      "cell_type": "code",
      "source": [
        "# Frecuencia de descripciones\n",
        "if 'Description' in df.columns:\n",
        "    desc_pd = df.groupBy('Description').count().orderBy('count', ascending=False).toPandas()\n",
        "    print(desc_pd.head(10))"
      ],
      "metadata": {
        "trusted": true,
        "id": "00139c3d"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bd4ac8b0",
      "cell_type": "code",
      "source": [
        "# Limpieza básica de texto: minúsculas, quitar signos de puntuación\n",
        "from pyspark.sql.functions import lower, regexp_replace, length\n",
        "if 'Description' in df.columns:\n",
        "    df = df.withColumn('Description_clean', lower(regexp_replace('Description', '[^a-zA-Z0-9 ]', '')))"
      ],
      "metadata": {
        "trusted": true,
        "id": "bd4ac8b0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "793ecea8",
      "cell_type": "code",
      "source": [
        "# Tokenización de palabras clave\n",
        "from pyspark.ml.feature import Tokenizer\n",
        "if 'Description_clean' in df.columns:\n",
        "    tokenizer = Tokenizer(inputCol=\"Description_clean\", outputCol=\"Description_tokens\")\n",
        "    df = tokenizer.transform(df)\n",
        "    tokens_pd = df.select('Description_tokens').limit(10000).toPandas()\n",
        "    from collections import Counter\n",
        "    import itertools\n",
        "    all_tokens = list(itertools.chain.from_iterable(tokens_pd['Description_tokens']))\n",
        "    word_freq = Counter(all_tokens)\n",
        "    print(word_freq.most_common(20))\n",
        "    from wordcloud import WordCloud\n",
        "    wc = WordCloud(width=800, height=400).generate_from_frequencies(word_freq)\n",
        "    plt.figure(figsize=(12,6))\n",
        "    plt.imshow(wc, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-06-01T17:00:58.936Z"
        },
        "id": "793ecea8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "010ed603",
      "cell_type": "code",
      "source": [
        "# Features simples: longitud, número de palabras, presencia de palabras clave\n",
        "from pyspark.sql.functions import size\n",
        "if 'Description_clean' in df.columns and 'Description_tokens' in df.columns:\n",
        "    df = df.withColumn('desc_length', length('Description_clean'))\n",
        "    df = df.withColumn('desc_num_words', size('Description_tokens'))\n",
        "    from pyspark.sql.functions import array_contains, lit\n",
        "    df = df.withColumn('desc_has_accident', array_contains('Description_tokens', lit('accident')))"
      ],
      "metadata": {
        "trusted": true,
        "id": "010ed603"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "15471661",
      "cell_type": "code",
      "source": [
        "# Vectorización básica: TF y TF-IDF\n",
        "from pyspark.ml.feature import CountVectorizer, IDF\n",
        "if 'Description_tokens' in df.columns:\n",
        "    cv = CountVectorizer(inputCol=\"Description_tokens\", outputCol=\"desc_tf\", vocabSize=1000, minDF=5)\n",
        "    cv_model = cv.fit(df)\n",
        "    df = cv_model.transform(df)\n",
        "    idf = IDF(inputCol=\"desc_tf\", outputCol=\"desc_tfidf\")\n",
        "    idf_model = idf.fit(df)\n",
        "    df = idf_model.transform(df)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "15471661"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "772dacfd-dae0-4a61-93ea-280d96bda77d",
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "trusted": true,
        "id": "772dacfd-dae0-4a61-93ea-280d96bda77d"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f474d158",
      "cell_type": "markdown",
      "source": [
        "# Feature Engineering y Pipeline de Preparación de Datos para ML en Spark\n",
        "\n",
        "Implementación el pipeline de preparación de datos para clasificación de severidad, usando PySpark, con ranking manual de Weather_Condition."
      ],
      "metadata": {
        "id": "f474d158"
      }
    },
    {
      "id": "6d91b32f",
      "cell_type": "code",
      "source": [
        "# Selección de features relevantes\n",
        "features_finales = [\n",
        "    'Distance(mi)', 'Precipitation(in)',\n",
        "    'Weather_Condition', 'State', 'hour', 'weekday',\n",
        "    'Sunrise_Sunset', 'Traffic_Signal', 'Crossing',\n",
        "    'desc_length', 'desc_num_words', 'desc_has_accident', 'Severity', 'desc_tf',\n",
        " 'desc_tfidf'\n",
        "]\n",
        "features_finales = [col for col in features_finales if col in df.columns]\n",
        "df_ml = df.select(*features_finales)"
      ],
      "metadata": {
        "trusted": true,
        "id": "6d91b32f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "5af83fb3",
      "cell_type": "code",
      "source": [
        "# Ranking manual de Weather_Condition con ranking único por condición\n",
        "from pyspark.sql.functions import monotonically_increasing_id\n",
        "\n",
        "# Obtener lista única de condiciones\n",
        "weather_conditions = df_ml.select('Weather_Condition').distinct().rdd.flatMap(lambda x: x).collect()\n",
        "weather_conditions = sorted(weather_conditions)\n",
        "\n",
        "very_good = [\n",
        "    'Clear', 'Fair', 'Fair / Windy', 'Mostly Clear', 'Sunny', 'Partly Cloudy', 'Partly Cloudy / Windy',\n",
        "    'Mostly Sunny', 'Scattered Clouds'\n",
        "]\n",
        "good = [\n",
        "    'Mostly Cloudy', 'Cloudy', 'Overcast', 'Overcast / Windy', 'Cloudy / Windy',\n",
        "    'Haze', 'Smoke'\n",
        "]\n",
        "moderate = [\n",
        "    'Light Rain', 'Rain Showers', 'Rain', 'Showers', 'Drizzle', 'Sprinkles',\n",
        "    'Light Rain / Windy', 'Rain / Windy', 'Light Drizzle',\n",
        "    'Light Freezing Rain', 'Light Snow Grains', 'Light Freezing Drizzle'\n",
        "]\n",
        "bad = [\n",
        "    'Heavy Rain', 'Heavy Rain / Windy', 'Rain Shower', 'Thunderstorms', 'Thunderstorm',\n",
        "    'Thunderstorms and Rain', 'Light Thunderstorms and Rain',\n",
        "    'Fog', 'Fog / Windy', 'Mist', 'Patches of Fog', 'Shallow Fog',\n",
        "    'Blowing Dust', 'Widespread Dust', 'Volcanic Ash'\n",
        "]\n",
        "very_bad = [\n",
        "    'Snow', 'Light Snow', 'Heavy Snow', 'Snow Showers', 'Blowing Snow',\n",
        "    'Freezing Rain', 'Freezing Drizzle', 'Ice Pellets', 'Sleet', 'Hail',\n",
        "    'Snow Grains', 'Small Hail', 'Freezing Fog', 'Heavy Freezing Rain',\n",
        "    'Heavy Thunderstorms and Rain', 'Unknown', 'Duststorm', 'Sandstorm'\n",
        "]\n",
        "\n",
        "# Crear ranking único por condición\n",
        "ranking = {}\n",
        "rank = 0\n",
        "for group in [very_good, good, moderate, bad, very_bad]:\n",
        "    for cond in group:\n",
        "        ranking[cond] = rank\n",
        "        rank += 1\n",
        "remaining_conditions = [cond for cond in weather_conditions if cond not in ranking]\n",
        "for cond in remaining_conditions:\n",
        "    ranking[cond] = rank\n",
        "    rank += 1\n",
        "\n",
        "# Crear un DataFrame de mapeo para Spark\n",
        "import pandas as pd\n",
        "ranking_pd = pd.DataFrame(list(ranking.items()), columns=['Weather_Condition', 'Weather_Condition_rank'])\n",
        "ranking_spark = spark.createDataFrame(ranking_pd)\n",
        "\n",
        "# Hacer join para asignar el ranking único\n",
        "from pyspark.sql.functions import coalesce\n",
        "\n",
        "df_ml = df_ml.join(ranking_spark, on='Weather_Condition', how='left')\n",
        "df_ml = df_ml.drop('Weather_Condition')"
      ],
      "metadata": {
        "trusted": true,
        "id": "5af83fb3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "01d5192d",
      "cell_type": "code",
      "source": [
        "# Encoding de variables categóricas restantes (StringIndexer + OneHotEncoder)\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Convertir columnas booleanas a string\n",
        "for c in ['Traffic_Signal', 'Crossing']:\n",
        "    if c in df_ml.columns and dict(df_ml.dtypes)[c] == 'boolean':\n",
        "        df_ml = df_ml.withColumn(c, col(c).cast('string'))\n",
        "\n",
        "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
        "from pyspark.ml import Pipeline\n",
        "cat_vars = ['State', 'hour', 'weekday', 'Sunrise_Sunset', 'Traffic_Signal', 'Crossing']\n",
        "cat_vars = [col for col in cat_vars if col in df_ml.columns]\n",
        "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_idx\", handleInvalid=\"keep\") for col in cat_vars]\n",
        "encoders = [OneHotEncoder(inputCol=col+\"_idx\", outputCol=col+\"_ohe\") for col in cat_vars]\n",
        "pipeline = Pipeline(stages=indexers + encoders)\n",
        "df_ml = pipeline.fit(df_ml).transform(df_ml)"
      ],
      "metadata": {
        "trusted": true,
        "id": "01d5192d"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "b5a4dc99",
      "cell_type": "code",
      "source": [
        "# Estandarización de variables numéricas\n",
        "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
        "num_vars = ['Distance(mi)', 'Precipitation(in)', 'desc_length', 'desc_num_words']\n",
        "num_vars = [col for col in num_vars if col in df_ml.columns]\n",
        "assembler_num = VectorAssembler(inputCols=num_vars, outputCol=\"num_features\")\n",
        "df_ml = assembler_num.transform(df_ml)\n",
        "scaler = StandardScaler(inputCol=\"num_features\", outputCol=\"num_features_scaled\")\n",
        "df_ml = scaler.fit(df_ml).transform(df_ml)"
      ],
      "metadata": {
        "trusted": true,
        "id": "b5a4dc99"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "95f9563f",
      "cell_type": "code",
      "source": [
        "# Incorporar features de texto TF-IDF\n",
        "from pyspark.ml.feature import VectorSlicer\n",
        "n_features_tfidf = 200\n",
        "tfidf_col = 'desc_tfidf'\n",
        "if tfidf_col in df_ml.columns:\n",
        "    slicer = VectorSlicer(inputCol=tfidf_col, outputCol=\"desc_tfidf_sliced\", indices=list(range(n_features_tfidf)))\n",
        "    df_ml = slicer.transform(df_ml)"
      ],
      "metadata": {
        "trusted": true,
        "id": "95f9563f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "bd352bde-40f9-48cc-acef-6117e498e39f",
      "cell_type": "code",
      "source": [
        "tfidf_col in df_ml.columns"
      ],
      "metadata": {
        "trusted": true,
        "id": "bd352bde-40f9-48cc-acef-6117e498e39f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e798765d",
      "cell_type": "code",
      "source": [
        "# División train/test (randomSplit, no estratifica pero es lo estándar en Spark)\n",
        "train_df, test_df = df_ml.randomSplit([0.8, 0.2], seed=42)\n",
        "print('Train count:', train_df.count(), 'Test count:', test_df.count())"
      ],
      "metadata": {
        "trusted": true,
        "id": "e798765d"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e4fee3fb",
      "cell_type": "code",
      "source": [
        "# Guardar dataset final para ML (Parquet)\n",
        "train_df.write.mode('overwrite').parquet('../data/train_ml.parquet')\n",
        "test_df.write.mode('overwrite').parquet('../data/test_ml.parquet')"
      ],
      "metadata": {
        "trusted": true,
        "id": "e4fee3fb"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}