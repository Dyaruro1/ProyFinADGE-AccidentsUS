{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12020309,"sourceType":"datasetVersion","datasetId":7562543}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"3f619533","cell_type":"markdown","source":"# Exploratory Data Analysis (EDA) Distribuido para ML en Spark/Cluster\nEste notebook realiza un EDA eficiente y escalable usando PySpark y librerías distribuidas como Vaex y Modin, ideales para grandes volúmenes de datos en clústeres Spark/Hadoop.","metadata":{}},{"id":"9889d2ca","cell_type":"code","source":"# Inicializar SparkSession\nfrom pyspark.sql import SparkSession\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nspark = SparkSession.builder.appName(\"US_Accidents_EDA\").getOrCreate()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T17:01:33.460509Z","iopub.execute_input":"2025-06-01T17:01:33.460842Z","iopub.status.idle":"2025-06-01T17:01:46.224363Z","shell.execute_reply.started":"2025-06-01T17:01:33.460787Z","shell.execute_reply":"2025-06-01T17:01:46.223145Z"}},"outputs":[{"name":"stderr","text":"Setting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n25/06/01 17:01:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n","output_type":"stream"}],"execution_count":1},{"id":"7d869797","cell_type":"code","source":"# Cargar el dataset en Spark DataFrame\ndf = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(\"/kaggle/input/muestra-accidents/muestra.csv\")\ndf.cache()\ndf.show(5)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7aef8779","cell_type":"code","source":"# Dimensiones y tipos de datos\ndf.printSchema()\nprint(f\"Filas: {df.count()}, Columnas: {len(df.columns)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"fb6caf0f","cell_type":"code","source":"# Estadísticas descriptivas\ndf.describe().show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f804caa9","cell_type":"markdown","source":"## Estadísticas descriptivas y distribución de la variable objetivo","metadata":{}},{"id":"4a505029","cell_type":"code","source":"# Conteo de severidad de accidentes (distribución de la variable objetivo)\nseverity_pd = df.groupBy('Severity').count().orderBy('Severity').toPandas()\nsns.barplot(x='Severity', y='count', data=severity_pd)\nplt.title('Distribución de Severidad de Accidentes')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"7471fe91","cell_type":"code","source":"# Estadísticas descriptivas de variables numéricas\nnum_features = [field.name for field in df.schema.fields if field.dataType.typeName() in ['integer', 'double']]\ndf.select(num_features).describe().show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"d1c5f113","cell_type":"markdown","source":"# Limpieza de valores nulos y preparacion de datos\n","metadata":{}},{"id":"2f22ca30","cell_type":"code","source":"from pyspark.sql.functions import col, count, when, lit, to_timestamp\n\n# Eliminar columnas con demasiados nulos\ncols_to_drop = [\n    'End_Lat', 'End_Lng', 'Precipitation(in)', 'Wind_Chill(F)', 'Airport_Code'\n]\ndf = df.drop(*cols_to_drop)\n\n# Imputar columnas numéricas con la mediana\ndef impute_median(df, cols):\n    for col_name in cols:\n        median_val = df.approxQuantile(col_name, [0.5], 0.01)[0] if col_name in df.columns else None\n        if median_val is not None:\n            df = df.withColumn(col_name, when(col(col_name).isNull(), lit(median_val)).otherwise(col(col_name)))\n    return df\n\nnum_cols = [\n    'Wind_Speed(mph)', 'Visibility(mi)', 'Humidity(%)', 'Temperature(F)', 'Pressure(in)'\n]\ndf = impute_median(df, num_cols)\n\n# Imputar columnas categóricas con la moda\nfrom pyspark.sql import functions as F\n\ndef impute_mode(df, cols):\n    for col_name in cols:\n        if col_name in df.columns:\n            mode_row = df.groupBy(col_name).count().orderBy(F.desc('count')).first()\n            mode_val = mode_row[0] if mode_row else 'Unknown'\n            df = df.withColumn(col_name, when(col(col_name).isNull(), lit(mode_val)).otherwise(col(col_name)))\n    return df\n\ncat_cols = [\n    'Weather_Condition', 'Wind_Direction', 'Weather_Timestamp',\n    'Nautical_Twilight', 'Civil_Twilight', 'Sunrise_Sunset', 'Astronomical_Twilight',\n    'Street', 'Timezone', 'Zipcode', 'City'\n]\ndf = impute_mode(df, cat_cols)\n\n# Verificar que no queden nulos \ndf.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns]).show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"02e87aac","cell_type":"code","source":"# Imputación de nulos en columnas de fecha con la mediana o valor fijo si la columna es nula\n\nfrom pyspark.sql.functions import col, when, lit, to_timestamp, unix_timestamp\n\ndef impute_datetime_median_spark(df, cols):\n    for col_name in cols:\n        if col_name in df.columns:\n            # Convertir a timestamp si es necesario\n            df = df.withColumn(col_name, to_timestamp(col(col_name)))\n            # Convertir a long \n            df = df.withColumn(col_name + '_long', unix_timestamp(col(col_name)))\n            notnull_count = df.filter(col(col_name + '_long').isNotNull()).count()\n            if notnull_count > 0:\n                median_val = df.approxQuantile(col_name + '_long', [0.5], 0.01)[0]\n                df = df.withColumn(\n                    col_name + '_long',\n                    when(col(col_name + '_long').isNull(), lit(median_val)).otherwise(col(col_name + '_long'))\n                )\n                # Volver a timestamp\n                df = df.withColumn(col_name, (col(col_name + '_long')).cast('timestamp'))\n            else:\n                df = df.withColumn(\n                    col_name,\n                    when(col(col_name).isNull(), lit('1970-01-01 00:00:00')).otherwise(col(col_name))\n                )\n            # Eliminar columna auxiliar\n            df = df.drop(col_name + '_long')\n    return df\n\nfecha_cols = ['Start_Time', 'End_Time', 'Weather_Timestamp']\ndf = impute_datetime_median_spark(df, fecha_cols)\n\n# Verificar que no queden nulos en fechas\ndf.select([F.count(F.when(col(c).isNull(), c)).alias(c) for c in fecha_cols]).show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5600a5e5","cell_type":"markdown","source":"# Visualización de EDA distribuido: Graficar resultados agregados de Spark\nPara visualizar los resultados de Spark, exportamos muestras o agregados a pandas y graficamos con matplotlib/seaborn. Esto permite análisis visual incluso en entornos distribuidos.","metadata":{}},{"id":"71a62759","cell_type":"code","source":"# Top 10 ciudades con más accidentes\ncity_pd = df.groupBy('City').count().orderBy('count', ascending=False).limit(10).toPandas()\nplt.figure(figsize=(10, 5))\nsns.barplot(x='City', y='count', data=city_pd)\nplt.title('Top 10 ciudades con más accidentes')\nplt.ylabel('Número de accidentes')\nplt.xticks(rotation=45)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3408c550","cell_type":"code","source":"# Top 15 estados con más accidentes\nstate_pd = df.groupBy('State').count().orderBy('count', ascending=False).limit(15).toPandas()\nplt.figure(figsize=(10, 5))\nsns.barplot(x='State', y='count', data=state_pd)\nplt.title('Accidentes por estado (Top 15)')\nplt.ylabel('Número de accidentes')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"14df86d9","cell_type":"code","source":"# Distribución de accidentes por franja horaria (requiere conversión de fecha)\nfrom pyspark.sql.functions import hour, col\nif 'Start_Time' in df.columns:\n    df = df.withColumn('hour', hour(col('Start_Time')))\n    hour_pd = df.groupBy('hour', 'Severity').count().orderBy('hour', 'Severity').toPandas()\n    plt.figure(figsize=(12, 6))\n    sns.barplot(x='hour', y='count', hue='Severity', data=hour_pd)\n    plt.title('Accidentes por hora y severidad')\n    plt.xlabel('Hora')\n    plt.ylabel('Número de accidentes')\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"655b3b4c","cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n\n# Ejemplo: Gráfico de barras de la distribución de severidad\nseverity_pd = df.groupBy('Severity').count().orderBy('Severity').toPandas()\nsns.barplot(x='Severity', y='count', data=severity_pd)\nplt.title('Distribución de Severidad de Accidentes')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"3d363d80","cell_type":"code","source":"# barplot de Distance(mi) vs Severity\nboxplot_pd = df.groupBy('Severity').agg(F.expr('percentile_approx(`Distance(mi)`, 0.5)').alias('median'),\n                                        F.avg('Distance(mi)').alias('mean')).orderBy('Severity').toPandas()\nplt.figure(figsize=(8, 4))\nsns.barplot(x='Severity', y='median', data=boxplot_pd)\nplt.title('Mediana de Distance(mi) por Severity')\nplt.ylabel('Distance(mi) (mediana)')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b742acca","cell_type":"code","source":"# Heatmap de Weather_Condition vs Severity (Top 10)\nweather_pd = df.groupBy('Weather_Condition', 'Severity').count().toPandas()\ntop_weather = weather_pd.groupby('Weather_Condition')['count'].sum().nlargest(10).index\nweather_pd = weather_pd[weather_pd['Weather_Condition'].isin(top_weather)]\nweather_pivot = weather_pd.pivot(index='Weather_Condition', columns='Severity', values='count').fillna(0)\nplt.figure(figsize=(10, 6))\nsns.heatmap(weather_pivot, annot=True, fmt='.0f', cmap='Blues')\nplt.title('Weather Condition vs Severity (Top 10)')\nplt.ylabel('Weather_Condition')\nplt.xlabel('Severity')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"ca347b65","cell_type":"code","source":"# Gráfico de barras de accidentes por hora y severidad\nif 'hour' in df.columns:\n    hour_pd = df.groupBy('hour', 'Severity').count().orderBy('hour', 'Severity').toPandas()\n    plt.figure(figsize=(12, 6))\n    sns.barplot(x='hour', y='count', hue='Severity', data=hour_pd)\n    plt.title('Accidentes por hora y severidad')\n    plt.xlabel('Hora')\n    plt.ylabel('Número de accidentes')\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"14c8cb27","cell_type":"code","source":"# barplots de variables numéricas vs Severity (además de Distance(mi))\nfor col in ['Temperature(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Speed(mph)']:\n    boxplot_pd = df.groupBy('Severity').agg(F.expr(f'percentile_approx(`{col}`, 0.5)').alias('median')).orderBy('Severity').toPandas()\n    plt.figure(figsize=(8, 4))\n    sns.barplot(x='Severity', y='median', data=boxplot_pd)\n    plt.title(f'Mediana de {col} por Severity')\n    plt.ylabel(f'{col} (mediana)')\n    plt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e779da2f","cell_type":"markdown","source":"# Eliminación de filas con outliers extremos en columnas numéricas\nFiltramos las filas que tienen valores mayores al percentil 99.5 en 'Distance(mi)', 'Wind_Speed(mph)' y 'Visibility(mi)' para mejorar la calidad del análisis y los modelos de ML.","metadata":{}},{"id":"0142922e","cell_type":"code","source":"cols_outliers = ['Distance(mi)', 'Wind_Speed(mph)', 'Visibility(mi)']\npercentile = 0.995  # Puedes ajustar este valor\nfrom pyspark.sql import functions as F\n\nfor col in cols_outliers:\n    if col in df.columns:\n        upper = df.approxQuantile(col, [percentile], 0.01)[0]\n        df = df.filter(F.col(col) <= upper)\n\nprint(f\"Filas restantes tras eliminar outliers en {cols_outliers}: {df.count()}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"a038b745","cell_type":"markdown","source":"# Feature Engineering sobre la columna 'Description'\nLa columna 'Description' contiene información textual sobre el accidente. Vamos a explorarla y extraer features útiles para modelos de ML.","metadata":{}},{"id":"00139c3d","cell_type":"code","source":"# Frecuencia de descripciones\nif 'Description' in df.columns:\n    desc_pd = df.groupBy('Description').count().orderBy('count', ascending=False).toPandas()\n    print(desc_pd.head(10))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"bd4ac8b0","cell_type":"code","source":"# Limpieza básica de texto: minúsculas, quitar signos de puntuación\nfrom pyspark.sql.functions import lower, regexp_replace, length\nif 'Description' in df.columns:\n    df = df.withColumn('Description_clean', lower(regexp_replace('Description', '[^a-zA-Z0-9 ]', '')))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"793ecea8","cell_type":"code","source":"# Tokenización de palabras clave\nfrom pyspark.ml.feature import Tokenizer\nif 'Description_clean' in df.columns:\n    tokenizer = Tokenizer(inputCol=\"Description_clean\", outputCol=\"Description_tokens\")\n    df = tokenizer.transform(df)\n    tokens_pd = df.select('Description_tokens').limit(10000).toPandas()\n    from collections import Counter\n    import itertools\n    all_tokens = list(itertools.chain.from_iterable(tokens_pd['Description_tokens']))\n    word_freq = Counter(all_tokens)\n    print(word_freq.most_common(20))\n    from wordcloud import WordCloud\n    wc = WordCloud(width=800, height=400).generate_from_frequencies(word_freq)\n    plt.figure(figsize=(12,6))\n    plt.imshow(wc, interpolation='bilinear')\n    plt.axis('off')\n    plt.show()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-06-01T17:00:58.936Z"}},"outputs":[],"execution_count":null},{"id":"010ed603","cell_type":"code","source":"# Features simples: longitud, número de palabras, presencia de palabras clave\nfrom pyspark.sql.functions import size\nif 'Description_clean' in df.columns and 'Description_tokens' in df.columns:\n    df = df.withColumn('desc_length', length('Description_clean'))\n    df = df.withColumn('desc_num_words', size('Description_tokens'))\n    from pyspark.sql.functions import array_contains, lit\n    df = df.withColumn('desc_has_accident', array_contains('Description_tokens', lit('accident')))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"15471661","cell_type":"code","source":"# Vectorización básica: TF y TF-IDF\nfrom pyspark.ml.feature import CountVectorizer, IDF\nif 'Description_tokens' in df.columns:\n    cv = CountVectorizer(inputCol=\"Description_tokens\", outputCol=\"desc_tf\", vocabSize=1000, minDF=5)\n    cv_model = cv.fit(df)\n    df = cv_model.transform(df)\n    idf = IDF(inputCol=\"desc_tf\", outputCol=\"desc_tfidf\")\n    idf_model = idf.fit(df)\n    df = idf_model.transform(df)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"772dacfd-dae0-4a61-93ea-280d96bda77d","cell_type":"code","source":"df.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"f474d158","cell_type":"markdown","source":"# Feature Engineering y Pipeline de Preparación de Datos para ML en Spark\n\nImplementación el pipeline de preparación de datos para clasificación de severidad, adaptando el flujo de Kaggle y usando PySpark, con ranking manual de Weather_Condition.","metadata":{}},{"id":"6d91b32f","cell_type":"code","source":"# Selección de features relevantes\nfeatures_finales = [\n    'Distance(mi)', 'Precipitation(in)',\n    'Weather_Condition', 'State', 'hour', 'weekday',\n    'Sunrise_Sunset', 'Traffic_Signal', 'Crossing',\n    'desc_length', 'desc_num_words', 'desc_has_accident', 'Severity', 'desc_tf',\n 'desc_tfidf'\n]\nfeatures_finales = [col for col in features_finales if col in df.columns]\ndf_ml = df.select(*features_finales)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"5af83fb3","cell_type":"code","source":"# Ranking manual de Weather_Condition con ranking único por condición (igual que Kaggle)\nfrom pyspark.sql.functions import monotonically_increasing_id\n\n# Obtener lista única de condiciones\nweather_conditions = df_ml.select('Weather_Condition').distinct().rdd.flatMap(lambda x: x).collect()\nweather_conditions = sorted(weather_conditions)\n\nvery_good = [\n    'Clear', 'Fair', 'Fair / Windy', 'Mostly Clear', 'Sunny', 'Partly Cloudy', 'Partly Cloudy / Windy',\n    'Mostly Sunny', 'Scattered Clouds'\n]\ngood = [\n    'Mostly Cloudy', 'Cloudy', 'Overcast', 'Overcast / Windy', 'Cloudy / Windy',\n    'Haze', 'Smoke'\n]\nmoderate = [\n    'Light Rain', 'Rain Showers', 'Rain', 'Showers', 'Drizzle', 'Sprinkles',\n    'Light Rain / Windy', 'Rain / Windy', 'Light Drizzle',\n    'Light Freezing Rain', 'Light Snow Grains', 'Light Freezing Drizzle'\n]\nbad = [\n    'Heavy Rain', 'Heavy Rain / Windy', 'Rain Shower', 'Thunderstorms', 'Thunderstorm',\n    'Thunderstorms and Rain', 'Light Thunderstorms and Rain',\n    'Fog', 'Fog / Windy', 'Mist', 'Patches of Fog', 'Shallow Fog',\n    'Blowing Dust', 'Widespread Dust', 'Volcanic Ash'\n]\nvery_bad = [\n    'Snow', 'Light Snow', 'Heavy Snow', 'Snow Showers', 'Blowing Snow',\n    'Freezing Rain', 'Freezing Drizzle', 'Ice Pellets', 'Sleet', 'Hail',\n    'Snow Grains', 'Small Hail', 'Freezing Fog', 'Heavy Freezing Rain',\n    'Heavy Thunderstorms and Rain', 'Unknown', 'Duststorm', 'Sandstorm'\n]\n\n# Crear ranking único por condición\nranking = {}\nrank = 0\nfor group in [very_good, good, moderate, bad, very_bad]:\n    for cond in group:\n        ranking[cond] = rank\n        rank += 1\nremaining_conditions = [cond for cond in weather_conditions if cond not in ranking]\nfor cond in remaining_conditions:\n    ranking[cond] = rank\n    rank += 1\n\n# Crear un DataFrame de mapeo para Spark\nimport pandas as pd\nranking_pd = pd.DataFrame(list(ranking.items()), columns=['Weather_Condition', 'Weather_Condition_rank'])\nranking_spark = spark.createDataFrame(ranking_pd)\n\n# Hacer join para asignar el ranking único\nfrom pyspark.sql.functions import coalesce\n\ndf_ml = df_ml.join(ranking_spark, on='Weather_Condition', how='left')\ndf_ml = df_ml.drop('Weather_Condition')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"01d5192d","cell_type":"code","source":"# Encoding de variables categóricas restantes (StringIndexer + OneHotEncoder)\nfrom pyspark.sql.functions import col\n\n# Convertir columnas booleanas a string\nfor c in ['Traffic_Signal', 'Crossing']:\n    if c in df_ml.columns and dict(df_ml.dtypes)[c] == 'boolean':\n        df_ml = df_ml.withColumn(c, col(c).cast('string'))\n\nfrom pyspark.ml.feature import StringIndexer, OneHotEncoder\nfrom pyspark.ml import Pipeline\ncat_vars = ['State', 'hour', 'weekday', 'Sunrise_Sunset', 'Traffic_Signal', 'Crossing']\ncat_vars = [col for col in cat_vars if col in df_ml.columns]\nindexers = [StringIndexer(inputCol=col, outputCol=col+\"_idx\", handleInvalid=\"keep\") for col in cat_vars]\nencoders = [OneHotEncoder(inputCol=col+\"_idx\", outputCol=col+\"_ohe\") for col in cat_vars]\npipeline = Pipeline(stages=indexers + encoders)\ndf_ml = pipeline.fit(df_ml).transform(df_ml)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"b5a4dc99","cell_type":"code","source":"# Estandarización de variables numéricas\nfrom pyspark.ml.feature import StandardScaler, VectorAssembler\nnum_vars = ['Distance(mi)', 'Precipitation(in)', 'desc_length', 'desc_num_words']\nnum_vars = [col for col in num_vars if col in df_ml.columns]\nassembler_num = VectorAssembler(inputCols=num_vars, outputCol=\"num_features\")\ndf_ml = assembler_num.transform(df_ml)\nscaler = StandardScaler(inputCol=\"num_features\", outputCol=\"num_features_scaled\")\ndf_ml = scaler.fit(df_ml).transform(df_ml)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"95f9563f","cell_type":"code","source":"# Incorporar features de texto TF-IDF\nfrom pyspark.ml.feature import VectorSlicer\nn_features_tfidf = 200  \ntfidf_col = 'desc_tfidf'\nif tfidf_col in df_ml.columns:\n    slicer = VectorSlicer(inputCol=tfidf_col, outputCol=\"desc_tfidf_sliced\", indices=list(range(n_features_tfidf)))\n    df_ml = slicer.transform(df_ml)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"bd352bde-40f9-48cc-acef-6117e498e39f","cell_type":"code","source":"tfidf_col in df_ml.columns","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e798765d","cell_type":"code","source":"# División train/test (randomSplit, no estratifica pero es lo estándar en Spark)\ntrain_df, test_df = df_ml.randomSplit([0.8, 0.2], seed=42)\nprint('Train count:', train_df.count(), 'Test count:', test_df.count())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"e4fee3fb","cell_type":"code","source":"# Guardar dataset final para ML (Parquet)\ntrain_df.write.mode('overwrite').parquet('../data/train_ml.parquet')\ntest_df.write.mode('overwrite').parquet('../data/test_ml.parquet')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}